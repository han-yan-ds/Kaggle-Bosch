{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This project examines patterns in manufacturing defects\n",
    "\n",
    "Data is provided by Bosch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and examine the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "tic = time.process_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training set has 1183747 records\n",
    "\n",
    "test set has 1023747 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'Bosch-Production-Line-Performance/'\n",
    "batch_size = 20000\n",
    "chunksize = 20000\n",
    "train_num_records = 1183747\n",
    "\n",
    "skiprows = list(range(1,train_num_records))\n",
    "np.random.shuffle(skiprows)\n",
    "skiprows = np.array(skiprows)[batch_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's make this dataset somewhat balanced:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, let's get the FAIL rows:\n",
    "\n",
    "INITIALLY: Because there's so few, I'll have to scour the entire dataset and get all of them for there to be enough of them\n",
    "\n",
    "THEN:  I saved the fails as a new CSV so I can load them more quickly in the future, rather than having to scour entire datasets again"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "train_numeric_fail = pd.read_csv(folder+'train_numeric.csv', nrows=1) #initialize\n",
    "train_categorical_fail = pd.read_csv(folder+'train_categorical.csv', nrows=1) #initialize\n",
    "train_date_fail = pd.read_csv(folder+'train_date.csv', nrows=1) #initialize\n",
    "\n",
    "\n",
    "for chunk in pd.read_csv(folder+'train_numeric.csv', chunksize=chunksize):\n",
    "    chunk.drop(chunk[chunk['Response']==0].index, inplace=True)\n",
    "    train_numeric_fail = train_numeric_fail.append(chunk)\n",
    "\n",
    "train_numeric_fail.drop(train_numeric_fail[train_numeric_fail['Response']==0].index, inplace=True) #remove dummy row from initialize\n",
    "print(train_numeric_fail.shape)\n",
    "\n",
    "\n",
    "for chunk in pd.read_csv(folder+'train_categorical.csv', chunksize=chunksize, low_memory=False):\n",
    "    train_categorical_fail = train_categorical_fail.append(chunk[chunk['Id'].isin(train_numeric_fail['Id'])])\n",
    "\n",
    "\n",
    "train_categorical_fail = train_categorical_fail[train_categorical_fail['Id'].isin(train_numeric_fail['Id'])] #remove dummy row from initialize\n",
    "print(train_categorical_fail.shape)\n",
    "   \n",
    "\n",
    "for chunk in pd.read_csv(folder+'train_date.csv', chunksize=chunksize, low_memory=False):\n",
    "    train_date_fail = train_date_fail.append(chunk[chunk['Id'].isin(train_numeric_fail['Id'])])\n",
    "    \n",
    "train_date_fail = train_date_fail[train_date_fail['Id'].isin(train_numeric_fail['Id'])] #remove dummy row from initialize\n",
    "print(train_date_fail.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6879, 970)\n",
      "(6879, 2141)\n",
      "(6879, 1157)\n"
     ]
    }
   ],
   "source": [
    "train_numeric_fail = pd.read_csv(folder+'train_numeric_fail.csv', low_memory=False)\n",
    "train_categorical_fail = pd.read_csv(folder+'train_categorical_fail.csv', low_memory=False)\n",
    "train_date_fail = pd.read_csv(folder+'train_date_fail.csv', low_memory=False)\n",
    "\n",
    "print(train_numeric_fail.shape)\n",
    "print(train_categorical_fail.shape)\n",
    "print(train_date_fail.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second, let's get the PASS rows:\n",
    "\n",
    "This is much easier, I'll just grab some random rows and remove the fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19883, 970)\n",
      "(19883, 2141)\n",
      "(19883, 1157)\n",
      "CPU times: user 42 s, sys: 4.1 s, total: 46.1 s\n",
      "Wall time: 47.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_numeric_pass = pd.read_csv(folder+'train_numeric.csv', skiprows=skiprows)\n",
    "train_categorical_pass = pd.read_csv(folder+'train_categorical.csv', skiprows=skiprows, low_memory=False)\n",
    "train_date_pass = pd.read_csv(folder+'train_date.csv', skiprows=skiprows)\n",
    "\n",
    "\n",
    "train_numeric_pass.drop(train_numeric_pass[train_numeric_pass['Response']==1].index, inplace=True)\n",
    "train_categorical_pass = train_categorical_pass[train_categorical_pass['Id'].isin(train_numeric_pass['Id'])]\n",
    "train_date_pass = train_date_pass[train_date_pass['Id'].isin(train_numeric_pass['Id'])]\n",
    "\n",
    "\n",
    "print(train_numeric_pass.shape)\n",
    "print(train_categorical_pass.shape)\n",
    "print(train_date_pass.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third, let's merge the PASS and FAIL into a simple SAMPLE dataframe\n",
    "\n",
    "Recall that we did Pass and Fail separately because Fails were so hard to find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26762, 2141)\n",
      "(26762, 970)\n",
      "(26762, 1157)\n"
     ]
    }
   ],
   "source": [
    "train_categorical_sample = pd.concat([train_categorical_pass, train_categorical_fail])\n",
    "train_numeric_sample = pd.concat([train_numeric_pass, train_numeric_fail])\n",
    "train_date_sample = pd.concat([train_date_pass, train_date_fail])\n",
    "\n",
    "print(train_categorical_sample.shape)\n",
    "print(train_numeric_sample.shape)\n",
    "print(train_date_sample.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourth, let's create the FEATURE dataframe by merging NUMERIC and CATEGORICAL\n",
    "\n",
    "Could be useful to simplify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26762, 3110)\n"
     ]
    }
   ],
   "source": [
    "# merge categorical and numeric together for total train_features_sample DataFrame\n",
    "train_feature_sample = pd.merge(train_categorical_sample, train_numeric_sample,on='Id')\n",
    "\n",
    "print(train_feature_sample.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And finally, let's delete the PASS and FAIL dataframes, since they're no more use to us\n",
    "\n",
    "Only useful for balancing the dataset, which we now have with the SAMPLE dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_categorical_fail, train_numeric_fail, train_date_fail\n",
    "del train_categorical_pass, train_numeric_pass, train_date_pass\n",
    "\n",
    "#del chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's now start to get combinations of stations\n",
    "\n",
    "Another notebook gets just the adjacent stations/features/dates.   This will get ALL OUT combinations.  So it'll be massive.\n",
    "\n",
    "Thus, I'll start with stations first, because there are only 51 stations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, create some functions to rename the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractLSF(string, isDate=False):\n",
    "    '''\n",
    "    Given a string in the Bosch Line/Station/Feature format (eg: L0_S0_F26), extract the Line number, Station number, and Feature number\n",
    "    \n",
    "    Input: String in Lx_Sy_Fz format\n",
    "    \n",
    "    Output:  Tuple of (x, y, z)\n",
    "    \n",
    "    Sidenote:  isDate will extract based on Lx_Sy_Dd format instead, returns tuple of (x, y, d)\n",
    "    '''\n",
    "    # first assert the format is correct:\n",
    "    splitStr = string.split('_')\n",
    "    if isDate:\n",
    "        prefixList = ['L', 'S', 'D']\n",
    "    else:\n",
    "        prefixList = ['L', 'S', 'F']\n",
    "        \n",
    "    assert len(splitStr) == 3, 'Not 3 substrings split by \"_\"s!'\n",
    "    for i in range(0, len(splitStr)):\n",
    "        assert splitStr[i][1:].isnumeric(), 'Not numeric following the single-letter prefix!'\n",
    "        assert splitStr[i][:1] == prefixList[i], 'Not the proper single-letter prefixes!  Did you use a Date format and forget to set isDate=True?'\n",
    "    \n",
    "    #now extract numbers:\n",
    "    tempList = []\n",
    "    \n",
    "    for substr in splitStr:\n",
    "        number = float(substr[1:])\n",
    "        if number%1 == 0.0:\n",
    "            tempList.append(int(number))\n",
    "        else:\n",
    "            tempList.append(number)\n",
    "        \n",
    "    return tuple(tempList)\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "\n",
    "def getListLSF(inputSeries, isDate=False):\n",
    "    '''\n",
    "    Given a series with fields of Bosch style Lx_Sy_Fz: Get the lines, stations, and features (or dates)\n",
    "    Input:  series (row) in a Bosch-style QC matrix\n",
    "    Output:  pandas dataframe of lines, stations, and features/dates\n",
    "    '''\n",
    "    \n",
    "    for field in inputSeries.index:\n",
    "        try:\n",
    "            myL, myS, myF = extractLSF(field, isDate=isDate)\n",
    "        except:\n",
    "            continue\n",
    "        listL.append(myL)\n",
    "        listS.append(myS)\n",
    "        listF.append(myF)\n",
    "    \n",
    "    return {'Lines': listL, 'Stations': listS, 'Features': listF}\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "\n",
    "def getListLSD(inputSeries):\n",
    "    return getListLSF(inputSeries, isDate=True)\n",
    "\n",
    "\n",
    "# All L/S/Fs------------------------------------------------------------------------------------------\n",
    "\n",
    "def get_lsf(lsf_df, isDate=False):\n",
    "    '''From a Bosch DF, extract all the Line/Station/Feature combinations in the fields\n",
    "    Outputs a Dataframe of columns Line, Station, Feature'''\n",
    "    if isDate:\n",
    "        lsf = pd.DataFrame(columns=['Line', 'Station', 'Date'])\n",
    "    else:\n",
    "        lsf = pd.DataFrame(columns=['Line', 'Station', 'Feature'])\n",
    "    \n",
    "    for field in lsf_df.columns:\n",
    "        try:\n",
    "            lsf = lsf.append(pd.DataFrame([list(extractLSF(field, isDate))], columns=lsf.columns))\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return lsf\n",
    "\n",
    "# ----------------------------------------------------\n",
    "\n",
    "def dataframeSpecificColumns(data, columnname, isDate=False):\n",
    "    '''Returns a dataframe of Bosch format, but reformat the columns to show ONLY feature, or ONLY station, etc\n",
    "    Input: data (dataframe), columnname\n",
    "    Output: dataframe with extracted feature/station/whatever'''\n",
    "        \n",
    "    cols = ['Id']\n",
    "    cols.extend(list(get_lsf(data, isDate)[columnname]))\n",
    "\n",
    "    if 'Response' in data.columns:\n",
    "        cols.append('Response')\n",
    "        \n",
    "    df = data.copy()\n",
    "    df.columns = cols\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second, let's create the dataframe with the renamed columns\n",
    "\n",
    "With Stations as columns... although I can change it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = 'Station'\n",
    "\n",
    "train = dataframeSpecificColumns(train_feature_sample, column)\n",
    "train_numeric = dataframeSpecificColumns(train_numeric_sample, column)\n",
    "\n",
    "\n",
    "#del train_date_sample\n",
    "#del train_feature_sample\n",
    "#del train_numeric_sample\n",
    "#del train_categorical_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third, in the interest of reducing the combinations even further, let's remove the columns with low correlations with Response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm checking correlations (vs. Response) in 2 ways:\n",
    "\n",
    "1) \"value\" actual values (for categorical, assigning values to numbers)\n",
    "\n",
    "2) \"boolean\" NaN vs non-NaN\n",
    "\n",
    "3) \"norm\" normalized values (for numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, get correlations with numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) create \"value\" for numeric... scales for the numbers are based on only the columns themselves\n",
    "\n",
    "train_numeric_value = train_numeric_sample.copy()\n",
    "train_numeric_value.fillna(value=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) creating \"boolean\" for numeric\n",
    "\n",
    "train_numeric_boolean = train_numeric_sample.copy()\n",
    "train_numeric_boolean[train_numeric_boolean.drop(['Id','Response'], axis=1).notnull()]=1\n",
    "train_numeric_boolean.fillna(value=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) create \"norm\" for numeric... so scales for the numbers are based on all columns\n",
    "\n",
    "train_numeric_norm = train_numeric_value.copy()\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "train_numeric_norm = normalize(train_numeric_value)\n",
    "train_numeric_norm = pd.DataFrame(train_numeric_norm, columns=train_numeric_value.columns, index=train_numeric_value.index)\n",
    "train_numeric_norm['Id'] = train_numeric_value['Id']\n",
    "train_numeric_norm['Response'] = train_numeric_value['Response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1_S24_F1604   -0.101818\n",
      "L1_S24_F1632   -0.101441\n",
      "L1_S24_F1695   -0.114737\n",
      "L1_S24_F1723   -0.130767\n",
      "L1_S24_F1846   -0.115725\n",
      "L3_S29_F3351   -0.124732\n",
      "L3_S29_F3458   -0.124732\n",
      "L3_S29_F3464    0.107424\n",
      "L3_S29_F3470    0.107424\n",
      "Response        1.000000\n",
      "dtype: float64\n",
      "L3_S32_F3850    0.271615\n",
      "L3_S33_F3855   -0.211041\n",
      "L3_S33_F3857   -0.211041\n",
      "L3_S33_F3859   -0.211041\n",
      "L3_S33_F3861   -0.211041\n",
      "L3_S33_F3863   -0.211041\n",
      "L3_S33_F3865   -0.211041\n",
      "L3_S33_F3867   -0.211041\n",
      "L3_S33_F3869   -0.211041\n",
      "L3_S33_F3871   -0.211041\n",
      "L3_S33_F3873   -0.211041\n",
      "L3_S34_F3876   -0.176664\n",
      "L3_S34_F3878   -0.176664\n",
      "L3_S34_F3880   -0.176664\n",
      "L3_S34_F3882   -0.176664\n",
      "Response        1.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "numericValueCorr = train_numeric_value.corrwith(train_numeric_value['Response'], axis=0)\n",
    "numericValueCorr = numericValueCorr[(numericValueCorr>0.1) | (numericValueCorr<-0.1)]\n",
    "\n",
    "numericBooleanCorr = train_numeric_boolean.corrwith(train_numeric_boolean['Response'], axis=0)\n",
    "numericBooleanCorr = numericBooleanCorr[(numericBooleanCorr>0.1) | (numericBooleanCorr<-0.1)]\n",
    "\n",
    "numericNormCorr = train_numeric_norm.corrwith(train_numeric_norm['Response'], axis=0)\n",
    "numericNormCorr = numericNormCorr[(numericNormCorr>0.1) | (numericNormCorr<-0.1)]\n",
    "\n",
    "print(numericValueCorr)\n",
    "print(numericBooleanCorr)\n",
    "del numericNormCorr #don't print numericNormCorr, there's no good correlations at all..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second, get correlations with categorical columns\n",
    "\n",
    "I'll have to factorize these values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 8s, sys: 4.6 s, total: 3min 12s\n",
      "Wall time: 3min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 1) create \"value\" for categorical... I'll have to factorize these numbers (column-encoded or whole-dataframe-encoded)\n",
    "\n",
    "train_categorical_factorize = train_categorical_sample.drop('Id',axis=1).apply(pd.factorize, axis=1)\n",
    "\n",
    "train_categorical_value = pd.DataFrame([list(item[0]) for item in train_categorical_factorize])\n",
    "train_categorical_value.insert(loc=0, value=np.array(train_categorical_sample['Id']), column='Id')\n",
    "train_categorical_value.columns = train_categorical_sample.columns\n",
    "train_categorical_value['Response'] = np.array(train_numeric_value['Response'])\n",
    "\n",
    "train_categorical_value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) creating \"boolean\" for categorical\n",
    "\n",
    "train_categorical_boolean = train_categorical_value.copy()\n",
    "train_categorical_boolean[train_categorical_value.drop('Id', axis=1)>-1]=1\n",
    "train_categorical_boolean[train_categorical_value.drop('Id', axis=1)==-1]=0\n",
    "train_categorical_boolean['Response'] = np.array(train_numeric_value['Response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L3_S32_F3851    0.268391\n",
      "L3_S32_F3854    0.265568\n",
      "Response        1.000000\n",
      "dtype: float64\n",
      "L3_S32_F3851    0.276364\n",
      "L3_S32_F3854    0.276364\n",
      "Response        1.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "categoricalValueCorr = train_categorical_value.corrwith(train_categorical_value['Response'], axis=0)\n",
    "categoricalValueCorr = categoricalValueCorr[(categoricalValueCorr>0.1) | (categoricalValueCorr<-0.1)]\n",
    "\n",
    "categoricalBooleanCorr = train_categorical_boolean.corrwith(train_categorical_boolean['Response'], axis=0)\n",
    "categoricalBooleanCorr = categoricalBooleanCorr[(categoricalBooleanCorr>0.1) | (categoricalBooleanCorr<-0.1)]\n",
    "\n",
    "print(categoricalValueCorr)\n",
    "print(categoricalBooleanCorr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Third, get correlations with date columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) create \"value\" for date... scales for the numbers are based on only the timestamps so it SHOULD be consistent throughout whole dataframe\n",
    "\n",
    "train_date_value = train_date_sample.copy()\n",
    "train_date_value.fillna(value=0, inplace=True)\n",
    "train_date_value['Response'] = train_numeric_sample['Response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) creating \"boolean\" for date\n",
    "\n",
    "train_date_boolean = train_date_sample.copy()\n",
    "train_date_boolean[train_date_boolean.drop('Id', axis=1).notnull()]=1\n",
    "train_date_boolean.fillna(value=0, inplace=True)\n",
    "train_date_boolean['Response'] = train_numeric_sample['Response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L3_S32_D3852    0.225223\n",
      "L3_S33_D3856   -0.162181\n",
      "L3_S33_D3858   -0.162181\n",
      "L3_S33_D3860   -0.162181\n",
      "L3_S33_D3862   -0.162181\n",
      "L3_S33_D3864   -0.162181\n",
      "L3_S33_D3866   -0.162181\n",
      "L3_S33_D3868   -0.162181\n",
      "L3_S33_D3870   -0.162181\n",
      "L3_S33_D3872   -0.162181\n",
      "L3_S33_D3874   -0.162181\n",
      "L3_S34_D3875   -0.145119\n",
      "L3_S34_D3877   -0.145119\n",
      "L3_S34_D3879   -0.145119\n",
      "L3_S34_D3881   -0.145119\n",
      "L3_S34_D3883   -0.145119\n",
      "Response        1.000000\n",
      "dtype: float64\n",
      "L3_S32_D3852    0.271615\n",
      "L3_S33_D3856   -0.211041\n",
      "L3_S33_D3858   -0.211041\n",
      "L3_S33_D3860   -0.211041\n",
      "L3_S33_D3862   -0.211041\n",
      "L3_S33_D3864   -0.211041\n",
      "L3_S33_D3866   -0.211041\n",
      "L3_S33_D3868   -0.211041\n",
      "L3_S33_D3870   -0.211041\n",
      "L3_S33_D3872   -0.211041\n",
      "L3_S33_D3874   -0.211041\n",
      "L3_S34_D3875   -0.176664\n",
      "L3_S34_D3877   -0.176664\n",
      "L3_S34_D3879   -0.176664\n",
      "L3_S34_D3881   -0.176664\n",
      "L3_S34_D3883   -0.176664\n",
      "Response        1.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "dateValueCorr = train_date_value.corrwith(train_date_value['Response'], axis=0)\n",
    "dateValueCorr = dateValueCorr[(dateValueCorr>0.1) | (dateValueCorr<-0.1)]\n",
    "\n",
    "dateBooleanCorr = train_date_boolean.corrwith(train_date_boolean['Response'], axis=0)\n",
    "dateBooleanCorr = dateBooleanCorr[(dateBooleanCorr>0.1) | (dateBooleanCorr<-0.1)]\n",
    "\n",
    "print(dateValueCorr)\n",
    "print(dateBooleanCorr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now I have narrowed down the columns to high correlation columns:\n",
    "\n",
    "And I can start working with combinations now!  FEATURE ENGINEERING!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "boolCorr = pd.concat([numericBooleanCorr, categoricalBooleanCorr, dateBooleanCorr]).drop('Response').abs().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "L3_S32_F3854    0.276364\n",
       "L3_S32_F3851    0.276364\n",
       "L3_S32_D3852    0.271615\n",
       "L3_S32_F3850    0.271615\n",
       "L3_S33_D3856    0.211041\n",
       "L3_S33_F3855    0.211041\n",
       "L3_S33_F3871    0.211041\n",
       "L3_S33_F3873    0.211041\n",
       "L3_S33_F3865    0.211041\n",
       "L3_S33_F3863    0.211041\n",
       "L3_S33_F3861    0.211041\n",
       "L3_S33_D3874    0.211041\n",
       "L3_S33_F3859    0.211041\n",
       "L3_S33_F3857    0.211041\n",
       "L3_S33_F3869    0.211041\n",
       "L3_S33_F3867    0.211041\n",
       "L3_S33_D3858    0.211041\n",
       "L3_S33_D3860    0.211041\n",
       "L3_S33_D3862    0.211041\n",
       "L3_S33_D3864    0.211041\n",
       "L3_S33_D3866    0.211041\n",
       "L3_S33_D3868    0.211041\n",
       "L3_S33_D3870    0.211041\n",
       "L3_S33_D3872    0.211041\n",
       "L3_S34_D3875    0.176664\n",
       "L3_S34_F3882    0.176664\n",
       "L3_S34_F3876    0.176664\n",
       "L3_S34_F3878    0.176664\n",
       "L3_S34_F3880    0.176664\n",
       "L3_S34_D3879    0.176664\n",
       "L3_S34_D3881    0.176664\n",
       "L3_S34_D3877    0.176664\n",
       "L3_S34_D3883    0.176664\n",
       "dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boolCorr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like from BoolCorr that I can use S34, S33, S32_F3850, and S32_F3851 as columns to test combinations of.\n",
    "\n",
    "So let's get every combination of these 4 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "combosize = 2\n",
    "\n",
    "boolCols = ['L3_S34_F3882', 'L3_S33_F3855', 'L3_S32_F3850', 'L3_S32_F3851']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('S34_F3882', 'S33_F3855'),\n",
       " ('S34_F3882', 'S32_F3850'),\n",
       " ('S34_F3882', 'S32_F3851'),\n",
       " ('S33_F3855', 'S32_F3850'),\n",
       " ('S33_F3855', 'S32_F3851'),\n",
       " ('S32_F3850', 'S32_F3851')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(combinations(boolCols, combosize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getComboColBooleanDf(df, colList, combosize):\n",
    "    '''Returns a version of DF that has columns that are Combinations of the original DF columns\n",
    "    colList = list of columns to be in combinations\n",
    "    comboSize = combination size\n",
    "    '''\n",
    "    from itertools import combinations\n",
    "        \n",
    "    df = df[colList]\n",
    "    combos = list(combinations(colList, combosize))\n",
    "        \n",
    "    comboDf = pd.DataFrame(np.zeros((df.shape[0], len(combos))))\n",
    "    comboDf.columns = [s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-d63dd6e68a83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_feature_sample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Response'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgetComboColBooleanDf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboolCols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/han/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3695\u001b[0m                                            \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3696\u001b[0m                                            \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3697\u001b[0;31m                                            errors=errors)\n\u001b[0m\u001b[1;32m   3698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3699\u001b[0m     @rewrite_axis_style_signature('mapper', [('copy', True),\n",
      "\u001b[0;32m/home/han/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3109\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3110\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3111\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/han/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   3142\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3143\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3144\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3146\u001b[0m         \u001b[0;31m# Case for non-unique axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/han/anaconda3/lib/python3.6/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mPY2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/han/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mreindex\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3564\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'axis'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3565\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3566\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3568\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mAppender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_shared_docs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reindex_axis'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0m_shared_doc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/han/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mreindex\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3687\u001b[0m         \u001b[0;31m# perform the reindex on the axes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3688\u001b[0m         return self._reindex_axes(axes, level, limit, tolerance, method,\n\u001b[0;32m-> 3689\u001b[0;31m                                   fill_value, copy).__finalize__(self)\n\u001b[0m\u001b[1;32m   3690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3691\u001b[0m     def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,\n",
      "\u001b[0;32m/home/han/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_reindex_axes\u001b[0;34m(self, axes, level, limit, tolerance, method, fill_value, copy)\u001b[0m\n\u001b[1;32m   3494\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3495\u001b[0m             frame = frame._reindex_columns(columns, method, copy, level,\n\u001b[0;32m-> 3496\u001b[0;31m                                            fill_value, limit, tolerance)\n\u001b[0m\u001b[1;32m   3497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3498\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'index'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/han/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_reindex_columns\u001b[0;34m(self, new_columns, method, copy, level, fill_value, limit, tolerance)\u001b[0m\n\u001b[1;32m   3519\u001b[0m         return self._reindex_with_indexers({1: [new_columns, indexer]},\n\u001b[1;32m   3520\u001b[0m                                            \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3521\u001b[0;31m                                            allow_dups=False)\n\u001b[0m\u001b[1;32m   3522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3523\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_reindex_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/han/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_reindex_with_indexers\u001b[0;34m(self, reindexers, fill_value, copy, allow_dups)\u001b[0m\n\u001b[1;32m   3808\u001b[0m                                                 \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3809\u001b[0m                                                 \u001b[0mallow_dups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_dups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3810\u001b[0;31m                                                 copy=copy)\n\u001b[0m\u001b[1;32m   3811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3812\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnew_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/han/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mreindex_indexer\u001b[0;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy)\u001b[0m\n\u001b[1;32m   4419\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4420\u001b[0m             new_blocks = self._slice_take_blocks_ax0(indexer,\n\u001b[0;32m-> 4421\u001b[0;31m                                                      fill_tuple=(fill_value,))\n\u001b[0m\u001b[1;32m   4422\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4423\u001b[0m             new_blocks = [blk.take_nd(indexer, axis=axis, fill_tuple=(\n",
      "\u001b[0;32m/home/han/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36m_slice_take_blocks_ax0\u001b[0;34m(self, slice_or_indexer, fill_tuple)\u001b[0m\n\u001b[1;32m   4499\u001b[0m                     blocks.append(blk.take_nd(blklocs[mgr_locs.indexer],\n\u001b[1;32m   4500\u001b[0m                                               \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_mgr_locs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4501\u001b[0;31m                                               fill_tuple=None))\n\u001b[0m\u001b[1;32m   4502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4503\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/han/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mtake_nd\u001b[0;34m(self, indexer, axis, new_mgr_locs, fill_tuple)\u001b[0m\n\u001b[1;32m   1252\u001b[0m             \u001b[0mfill_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m             new_values = algos.take_nd(values, indexer, axis=axis,\n\u001b[0;32m-> 1254\u001b[0;31m                                        allow_fill=False)\n\u001b[0m\u001b[1;32m   1255\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m             \u001b[0mfill_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfill_tuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/han/anaconda3/lib/python3.6/site-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mtake_nd\u001b[0;34m(arr, indexer, axis, out, fill_value, mask_info, allow_fill)\u001b[0m\n\u001b[1;32m   1653\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'F'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1654\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1655\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1657\u001b[0m     func = _get_take_nd_function(arr.ndim, arr.dtype, out.dtype, axis=axis,\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test = train_feature_sample.drop(['Id', 'Response'], axis=1)\n",
    "\n",
    "\n",
    "getComboColBooleanDf(test, boolCols, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------Testing Combos Codes-----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getListFrequency(item, listOfItems):\n",
    "    count = 0\n",
    "    for i in listOfItems:\n",
    "        if i == item:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def getConnectionFrequency(inputList, allConnections):\n",
    "    \n",
    "    frequencies = np.zeros(len(allConnections))\n",
    "\n",
    "    for i in range(0, len(allConnections)):\n",
    "        item = allConnections[i]\n",
    "        frequencies[i] = getListFrequency(item, inputList)#number of allConnections[i] in inputList\n",
    "    \n",
    "    return frequencies.reshape(1,len(allConnections))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# This is when I can enter the Connection Combo Size that I want to test\n",
    "\n",
    "comboSize = 1\n",
    "wantUniqueConnections = True\n",
    "#-------------------------------------\n",
    "\n",
    "def getConnections_Custom_Uniques(inputSeries):\n",
    "    return getConnections(inputSeries, combosize=comboSize, unique=wantUniqueConnections, stringOutput=True)\n",
    "\n",
    "seriesConnections = train_feature_sample_stationcols.apply(getConnections_Custom_Uniques, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#TESTING\n",
    "# Creating a SET of all connections for the columns for my Connections DF... then converting to a list\n",
    "\n",
    "allConnections = set(())\n",
    "\n",
    "for product in seriesConnections:\n",
    "    allConnections.update(product)\n",
    "\n",
    "allConnections = list(allConnections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "connectionFrequency = pd.DataFrame(np.zeros((seriesConnections.size, len(allConnections))), columns=pd.Series(allConnections), index=seriesConnections.index)\n",
    "\n",
    "for row in seriesConnections.index:\n",
    "    connectionFrequency.loc[row] = getConnectionFrequency(seriesConnections[row], allConnections)\n",
    "\n",
    "results = train_feature_sample_stationcols['Response']\n",
    "\n",
    "print(connectionFrequency.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction #1\n",
    "\n",
    "### Principle Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaledData = scaler.fit_transform(connectionFrequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 2\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "pca.fit(scaledData)\n",
    "\n",
    "scaledData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pca = pca.transform(scaledData)\n",
    "\n",
    "x_pca.shape #I just reduced MANY dimensions to just 2!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(x_pca[:,0],x_pca[:,1],c=results,cmap='cool', s=15, alpha=0.5)\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction #2\n",
    "\n",
    "### Independent Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import FastICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 2\n",
    "\n",
    "ica = FastICA(n_components=n_components)\n",
    "ica.fit(scaledData)\n",
    "\n",
    "scaledData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ica = ica.transform(scaledData)\n",
    "\n",
    "x_ica.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(x_ica[:,0],x_ica[:,1],c=results,cmap='cool', s=15, alpha=0.5)\n",
    "plt.xlabel('First Independent Component')\n",
    "plt.ylabel('Second Independent Component')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction #3\n",
    "\n",
    "### t-Distributed Stochastic Neighbor Embedding (t-SNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2018/08/dimensionality-reduction-techniques-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 300\n",
    "\n",
    "tsne = TSNE(n_components=2, n_iter=n_iter).fit(scaledData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, n_iter=n_iter).fit_transform(scaledData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.title('t-SNE components')\n",
    "plt.scatter(tsne[:,0], tsne[:,1])\n",
    "plt.scatter(tsne[:,1], tsne[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction #4\n",
    "\n",
    "### Uniform Manifold Approximation and Projection (UMAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------------------------Random Forest --------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "(X_train, X_test, y_train, y_test) = train_test_split(x_pca, results, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = RandomForestClassifier(n_estimators=60)\n",
    "tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = tree.feature_importances_\n",
    "\n",
    "indices = np.argsort(importances[-9:]) #top 10 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = connectionFrequency.columns\n",
    "\n",
    "plt.title('Feature Importances')\n",
    "\n",
    "plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "plt.xlabel('Relative Importance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_tree = tree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, y_predict_tree))\n",
    "print(classification_report(y_test, y_predict_tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------Support Vector Machines-----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_svm = svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, y_predict_svm))\n",
    "print(classification_report(y_test, y_predict_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
